# Scan2Read
*you scan we read...exclusively for the visually impaired*
[![License](https://img.shields.io/badge/license-All%20Rights%20Reserved-red.svg)](LICENSE)

## Introduction
Scan2Read is a mobile application framework designed to facilitate the conversion of images to text and subsequently to speech. It aims to assist visually impaired individuals by providing them with a tool to extract text from images and convert it into speech for easy comprehension. This project was developed as a runner-up submission for a Hackathon, completed within 48 hours.

## Features
- **Image to Text Conversion**: Utilizes pytesseract, a Python library for Optical Character Recognition (OCR), to extract text from images.
- **Text to Speech Synthesis**: Integrates gTTS (Google Text-to-Speech), enabling the conversion of extracted text into audible speech.
- **User-Friendly Interface**: Built using Kivy, a Python framework for developing multitouch applications, ensuring accessibility for visually impaired users.
- **Accessibility-Focused Design**: Tailored exclusively for visually impaired individuals, with intuitive features and easy navigation.

## Installation
Currently, Scan2Read is not available on any app store. However, you can set it up locally by following these steps:

1. Clone the Scan2Read repository from GitHub.
2. Install the required dependencies:
pip install kivy pytesseract gTTS
3. Run the application using Python:
python main.py

## Usage
1. Launch the Scan2Read application on your mobile device.
2. Capture an image containing text using your device's camera.
3. Select the option to convert the image to text.
4. The extracted text will be displayed on the screen.
5. Optionally, choose to convert the text to speech.
6. The synthesized speech will be played through your device's speakers.

## Contributions and Support
If you have any suggestions, queries, or would like to collaborate on enhancing Scan2Read, feel free to reach out to us at [swastikbanerjee2001@gmail.com](mailto:swastikbanerjee2001@gmail.com). Your feedback and contributions are highly appreciated.
